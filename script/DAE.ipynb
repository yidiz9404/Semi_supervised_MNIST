{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle \n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded\n",
      "3000\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "from sub import subMNIST       # testing the subclass of MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Training Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset_import = pickle.load(open(\"train_labeled.p\", \"rb\"))\n",
    "validset_import = pickle.load(open(\"validation.p\", \"rb\"))\n",
    "trainset_unl = pickle.load(open(\"train_unlabeled.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset_import, batch_size=64, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(validset_import, batch_size=64, shuffle=True)\n",
    "train_unl_loader = torch.utils.data.DataLoader(trainset_unl, batch_size=64, shuffle=True)\n",
    "#train_loader.dataset.train_data[0]\n",
    "#train_loader.dataset.train_labels[0]\n",
    "#train_loader.dataset.train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "class Noise():\n",
    "    def SaltAndPepper(self, X, rate=0.3):\n",
    "        # Salt and pepper noise\n",
    "        X = X.numpy()\n",
    "        drop = X.shape[1]\n",
    "        numpy.random.shuffle(drop)\n",
    "        #drop = X.size()[1]\n",
    "        #input = Variable(torch.randn(1, 1, 28, 28))\n",
    "        sep = int(len(drop)*rate)\n",
    "        drop = drop[:sep]\n",
    "        X[:, drop[:sep/2]]=0\n",
    "        X[:, drop[sep/2:]]=1\n",
    "        X = torch.from_numpy(X)\n",
    "        return X\n",
    "        \n",
    "    def GaussianNoise(self, X, sd=0.5):\n",
    "        # Injecting small gaussian noise\n",
    "        X = X.numpy()\n",
    "        X += numpy.random.normal(0, sd, X.shape)\n",
    "        X = torch.from_numpy(X)\n",
    "        return X\n",
    "        \n",
    "    def MaskingNoise(self, X, rate=0.5):\n",
    "        mask = (numpy.random.uniform(0,1, X.shape)<rate).astype(\"i4\")\n",
    "        X = mask*X\n",
    "        return X\n",
    "        \n",
    "def SaltAndPepper(rate=0.3):\n",
    "    # Salt and pepper noise\n",
    "    def func(X):\n",
    "        drop = numpy.random.uniform(0,1, X.shape)\n",
    "        z = numpy.where(drop < 0.5*rate)\n",
    "        o = numpy.where(numpy.abs(drop - 0.75*rate) < 0.25*rate)\n",
    "        X[z]=0\n",
    "        X[o]=1   \n",
    "        return X\n",
    "    return func\n",
    "    \n",
    "def GaussianNoise(self, sd=0.5):\n",
    "    # Injecting small gaussian noise\n",
    "    def func(X):\n",
    "        X += numpy.random.normal(0, sd, X.shape)\n",
    "        return X\n",
    "    return func\n",
    "\n",
    "Noise = Noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DenoisingAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A denoising autoencoder will corrupt an input (add noise) and try to reconstruct it.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        # Define some model hyperparameters to work with MNIST images!\n",
    "        input_size  = [784, 20] # dimensions of image\n",
    "        out_size = [20,784]\n",
    "        hidden_size = 1000  # number of hidden units - generally bigger than input size for DAE\n",
    "\n",
    "        # Now, define the symbolic input to the model (Theano)\n",
    "        # We use a matrix rather than a vector so that minibatch processing can be done in parallel.\n",
    "        #x = T.fmatrix('x')\n",
    "        #self.inputs = [x]\n",
    "        self.fc1 = nn.Linear(input_size[0], input_size[1])\n",
    "        self.fc2 = nn.Linear(out_size[0],out_size[1])\n",
    "\n",
    "        # Build the model's parameters - a weight matrix and two bias vectors\n",
    "        #self.W  = nn.(input_size[0],input_size[1])\n",
    "\n",
    "#         # Perform the computation for a denoising autoencoder!\n",
    "#         # first, add noise (corrupt) the input\n",
    "#         corrupted_input = Noise.GaussianNoise(x, 0.5)\n",
    "#         # next, compute the hidden layer given the inputs (the encoding function)\n",
    "#         h1 = torch.tanh(torch.dot(corrupted_input, W))\n",
    "#         # finally, create the reconstruction from the hidden layer (we tie the weights with W.T)\n",
    "#         self.rec = torch.sigmoid(torch.dot(h1, W.T))\n",
    "#         # the training cost is reconstruction error - with MNIST this is binary cross-entropy\n",
    "#         self.train_cost = torch.binary_crossentropy(self.rec, x)\n",
    "\n",
    "        # Compile everything into a Theano function for prediction!\n",
    "        # When using real-world data in predictions, we wouldn't corrupt the input first.\n",
    "        # Therefore, create another version of the hiddens and reconstruction without adding the noise\n",
    "#         hiddens_predict = torch.tanh(torch.add(torch.mul(x, W),b1))\n",
    "#         recon_predict   = torch.sigmoid(torch.add(torch.mul(h1, y.T),b2))\n",
    "#         self.output     = recon_predict\n",
    "        \n",
    "    def decode(self, z, y):\n",
    "        #rec = torch.sigmoid(torch.cat((z, y)+b2))  # concatenate z and y\n",
    "        return self.rec\n",
    "    \n",
    "    def get_train_cost(self):\n",
    "        #torch.binary_crossentropy(self.rec)\n",
    "        return self.train_cost\n",
    "    \n",
    "    def get_inputs(self):\n",
    "        return self.inputs\n",
    "\n",
    "    # def get_params(self):\n",
    "    #    return self.params\n",
    "\n",
    "#     def get_outputs(self):\n",
    "#         return self.output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Perform the computation for a denoising autoencoder!\n",
    "        # first, add noise (corrupt) the input\n",
    "        #corrupted_input = self.Noise.GaussianNoise(x, 0.5)\n",
    "        # next, compute the hidden layer given the inputs (the encoding function)\n",
    "        x = x.data\n",
    "        x = x.view(-1,784)\n",
    "        #print(x)\n",
    "        corrupted_x = Noise.GaussianNoise(x, 0.5)\n",
    "        corrupted_x = Variable(corrupted_x )\n",
    "        x = Variable(x)\n",
    "        h1 = F.sigmoid(self.fc1(corrupted_x))\n",
    "        #print(h1)\n",
    "        # finally, create the reconstruction from the hidden layer (we tie the weights with W.T)\n",
    "        self.rec = F.sigmoid(self.fc2(h1))\n",
    "        \n",
    "        #print(self.rec)\n",
    "        self.train_cost = F.binary_cross_entropy(self.rec, x)\n",
    "        #print(self.train_cost)\n",
    "        return self.rec\n",
    "        # the training cost is reconstruction error - with MNIST this is binary cross-entropy\n",
    "#         self.train_cost = torch.binary_crossentropy(self.rec, x)\n",
    "#         hiddens_predict = torch.tanh(self.fc1(x))\n",
    "#         recon_predict   = torch.sigmoid(self.fc1(hiddens_predict))\n",
    "#         self.output     = recon_predict\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dae = DenoisingAutoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#w = torch.zeros(278,20)\n",
    "#w = Variable(w)\n",
    "def train(epoch):\n",
    "    dae.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #print(data)\n",
    "        #data = Noise.GaussianNoise(data, 0.5)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = dae(data)\n",
    "        loss = dae.get_train_cost()\n",
    "        #print(loss)\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test(epoch, valid_loader):\n",
    "    dae.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = dae(data)\n",
    "        test_loss += dae.get_train_cost().data[0]\n",
    "        pred = output.data # get the index of the max log-probability\n",
    "        #print(pred)\n",
    "        data = data.view(-1,784)\n",
    "        #print(data.data)\n",
    "        correct += pred.eq(data.data).cpu().sum()\n",
    "#         ind = output.data.max(1)[1]\n",
    "#         actual = data.data.numpy()\n",
    "#         if ind == actual:\n",
    "#             correct += 1\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#     print(data)\n",
    "#print(train_loader.dataset.train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 12 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     2\n",
       "    0     0     0     0     0     0     0     0     0     0     0    14   194\n",
       "    0     0     0     0     0     0     0     0     0     0     0    43   196\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0    19\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0    34\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0   111\n",
       "    0     0     0     0     0     0     0     0     0     0     0     9   155\n",
       "    0     0     0     0     0     0     0     0     0    91   158   224   254\n",
       "    0     0     0     0     0     0     0     0    11   230   254   254   254\n",
       "    0     0     0     0     0     0     0     0     1   107   254   217   132\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 13 to 25 \n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "   72   170   255   121     0     0     0     0     0     0     0     0     0\n",
       "  254   254   254   121     0     0     0     0     0     0     0     0     0\n",
       "  201   205   254   121     0     0     0     0     0     0     0     0     0\n",
       "    0   158   254   115     0     0     0     0     0     0     0     0     0\n",
       "    0   209   254    37     0     0     0     0     0     0     0     0     0\n",
       "   17   228   254    37     0     0     0     0     0     0     0     0     0\n",
       "  103   254   218     9     0     0     0     0     0     0     0     0     0\n",
       "  122   254   197     0     0     0     0     0     0     0     0     0     0\n",
       "  122   254   125     0     0     0     0     0     0     0     0     0     0\n",
       "  122   254   125     0     0     0     0     0     0     0     0     0     0\n",
       "  122   254    55     0     0     0     0     0     0     0     0     0     0\n",
       "  165   254    41     0     0     0     0     0     0     0     0     0     0\n",
       "  204   254    41     0     0     0     0     0     0     0     0     0     0\n",
       "  232   217     5     0     0     0     0     0     0     0     0     0     0\n",
       "  254   164     0     0     0     0     0     0     0     0     0     0     0\n",
       "  254   128     0     0     0     0     0     0     0     0     0     0     0\n",
       "  254   205   154   154   110    72    72     6     0     0     0     0     0\n",
       "  254   254   254   254   254   254   254   152     0     0     0     0     0\n",
       "  216   133    92    92   180   254   254    81     0     0     0     0     0\n",
       "   18     0     0     0     1     4    31     1     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
       "\n",
       "Columns 26 to 27 \n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "[torch.ByteTensor of size 28x28]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = train_unl_loader.dataset.train_data[0]\n",
    "Variable(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    dae.train()\n",
    "    #output = np.array([None]*47000)\n",
    "    output = []\n",
    "    for batch_idx, _ in enumerate(train_unl_loader.dataset.train_data):\n",
    "        #print(data)\n",
    "        #data = Noise.GaussianNoise(data, 0.5)\n",
    "        data = Variable(train_unl_loader.dataset.train_data[batch_idx].float())\n",
    "        recx = dae(data).data.view(-1,1,28,28)\n",
    "        output.append(recx.numpy())\n",
    "        loss = dae.get_train_cost()\n",
    "        #print(loss)\n",
    "        optimizer.step()\n",
    "        #if batch_idx % 10 == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx, len(train_unl_loader.dataset),\n",
    "#                 100. * batch_idx/len(train_unl_loader.dataset), loss.data[0]))\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(dae.parameters(), lr=0.01, momentum=0.5)####tune to improve\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, 2):\n",
    "    output = train(epoch)\n",
    "    #test(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = np.array(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = torch.from_numpy(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'map' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-da68898d60b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# plt.imshow(output[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# print images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'map' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "%matplotlib inline\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "# plt.imshow(output[0])\n",
    "dataiter = iter(output)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rec_unl_loader = torch.utils.data.DataLoader(output, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(sub.subMNIST, torch.FloatTensor)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_unl_loader.dataset),type(rec_unl_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,0 ,.,.) = \n",
       "\n",
       "Columns 0 to 8 \n",
       "   0.4982  0.4514  0.3015  0.4083  0.3829  0.5500  0.4334  0.5086  0.4684\n",
       "  0.2287  0.5547  0.5381  0.4233  0.6115  0.6136  0.2904  0.5718  0.4826\n",
       "  0.5525  0.6700  0.6324  0.5423  0.4645  0.6429  0.5358  0.6280  0.4171\n",
       "  0.3916  0.6583  0.6712  0.6460  0.4110  0.6812  0.3656  0.5168  0.4676\n",
       "  0.3646  0.4956  0.4400  0.6242  0.7213  0.3757  0.7069  0.5899  0.6013\n",
       "  0.4192  0.5719  0.3901  0.5345  0.5284  0.6441  0.5046  0.5397  0.4160\n",
       "  0.5534  0.6092  0.5978  0.6490  0.3634  0.4754  0.5372  0.3930  0.5708\n",
       "  0.4739  0.4341  0.2849  0.5558  0.2873  0.4786  0.4012  0.6506  0.4884\n",
       "  0.4363  0.3289  0.5188  0.4698  0.3814  0.6083  0.5868  0.3746  0.5309\n",
       "  0.3861  0.5343  0.3776  0.3565  0.5170  0.5617  0.4711  0.4798  0.4316\n",
       "  0.4309  0.3999  0.6717  0.3832  0.5260  0.3638  0.6046  0.4092  0.5032\n",
       "  0.5163  0.4687  0.3069  0.4846  0.3419  0.4203  0.5063  0.3722  0.2879\n",
       "  0.5059  0.3571  0.6907  0.5270  0.5415  0.6046  0.3679  0.4495  0.3620\n",
       "  0.5771  0.6505  0.6045  0.6798  0.3940  0.4293  0.4097  0.6811  0.5383\n",
       "  0.5352  0.5051  0.3735  0.4933  0.4156  0.5928  0.5483  0.4836  0.4869\n",
       "  0.3716  0.3380  0.4978  0.5399  0.4815  0.6695  0.5194  0.6172  0.4722\n",
       "  0.3527  0.4973  0.3791  0.4838  0.5559  0.4679  0.4780  0.5691  0.4325\n",
       "  0.4335  0.4296  0.5407  0.2156  0.3879  0.3965  0.5736  0.3344  0.4064\n",
       "  0.3982  0.3444  0.3755  0.4980  0.4416  0.5860  0.3460  0.3845  0.5491\n",
       "  0.5558  0.5272  0.3428  0.5281  0.4487  0.6370  0.4777  0.3813  0.5317\n",
       "  0.6312  0.2861  0.3352  0.3983  0.6676  0.5645  0.5019  0.5315  0.5623\n",
       "  0.3657  0.5935  0.4753  0.5421  0.5004  0.5103  0.5908  0.4091  0.4921\n",
       "  0.4832  0.7330  0.3082  0.3159  0.4660  0.1434  0.5420  0.6647  0.4684\n",
       "  0.4438  0.6769  0.3629  0.4124  0.3660  0.6889  0.5589  0.5073  0.6770\n",
       "  0.3768  0.5762  0.5541  0.7353  0.4210  0.7204  0.6852  0.7301  0.4400\n",
       "  0.5795  0.5028  0.4882  0.4339  0.5545  0.3010  0.5990  0.6475  0.5475\n",
       "  0.4668  0.5157  0.4752  0.6151  0.4191  0.4430  0.3149  0.3600  0.3673\n",
       "  0.4772  0.6250  0.4612  0.3034  0.4617  0.5567  0.5013  0.6594  0.5162\n",
       "\n",
       "Columns 9 to 17 \n",
       "   0.3574  0.4539  0.4164  0.6349  0.5358  0.3578  0.6171  0.6481  0.4472\n",
       "  0.5380  0.5714  0.4607  0.3094  0.5038  0.5238  0.4549  0.5421  0.4762\n",
       "  0.4822  0.5611  0.6432  0.6124  0.3188  0.4789  0.5856  0.6372  0.4917\n",
       "  0.5759  0.6586  0.4506  0.4948  0.6111  0.4768  0.3642  0.5419  0.4666\n",
       "  0.5083  0.5606  0.6275  0.5253  0.6337  0.5032  0.5122  0.6655  0.5207\n",
       "  0.5061  0.3863  0.4104  0.6561  0.6429  0.5473  0.7164  0.5525  0.3535\n",
       "  0.5295  0.5897  0.6128  0.3924  0.6026  0.6040  0.4850  0.3682  0.3336\n",
       "  0.6917  0.6078  0.4655  0.5873  0.5261  0.5010  0.5245  0.5673  0.4500\n",
       "  0.3669  0.6480  0.2923  0.5560  0.6498  0.4388  0.6369  0.5973  0.6567\n",
       "  0.3455  0.4550  0.3441  0.6107  0.4616  0.4938  0.4378  0.3965  0.5070\n",
       "  0.4212  0.3433  0.6158  0.4285  0.3024  0.4898  0.4070  0.5845  0.4444\n",
       "  0.3870  0.7422  0.3656  0.5384  0.5726  0.5232  0.3948  0.5696  0.3496\n",
       "  0.5183  0.5279  0.4269  0.5472  0.5733  0.5973  0.5009  0.2582  0.3333\n",
       "  0.5408  0.4293  0.3953  0.6195  0.3473  0.3578  0.5568  0.5451  0.3572\n",
       "  0.5459  0.5826  0.5230  0.4285  0.3737  0.4505  0.4332  0.3636  0.6567\n",
       "  0.5034  0.4582  0.3634  0.5744  0.4632  0.5494  0.4314  0.7184  0.4021\n",
       "  0.4604  0.4630  0.4774  0.4714  0.2293  0.5330  0.5183  0.5810  0.4752\n",
       "  0.3238  0.4137  0.3853  0.4215  0.4033  0.4575  0.5218  0.5011  0.4395\n",
       "  0.4114  0.5192  0.4102  0.6057  0.3843  0.5152  0.5686  0.6419  0.5412\n",
       "  0.6126  0.5162  0.5594  0.4163  0.5801  0.6880  0.4070  0.6393  0.4103\n",
       "  0.4615  0.5815  0.5140  0.3130  0.5366  0.7578  0.6124  0.6254  0.4711\n",
       "  0.4828  0.4792  0.3679  0.5755  0.5181  0.4323  0.5363  0.4521  0.4064\n",
       "  0.4885  0.6897  0.6497  0.5488  0.5252  0.5995  0.6587  0.5655  0.5145\n",
       "  0.5453  0.4940  0.4498  0.6331  0.6769  0.5123  0.3813  0.5687  0.6293\n",
       "  0.5168  0.5738  0.4423  0.6095  0.5180  0.8250  0.4977  0.3487  0.4659\n",
       "  0.2488  0.4089  0.5408  0.6359  0.3778  0.5968  0.4686  0.5167  0.5637\n",
       "  0.4225  0.3916  0.4660  0.4825  0.4602  0.5131  0.5299  0.5034  0.6096\n",
       "  0.3877  0.3094  0.4917  0.5698  0.3616  0.6784  0.6771  0.5567  0.5214\n",
       "\n",
       "Columns 18 to 26 \n",
       "   0.5296  0.4485  0.5735  0.4277  0.2783  0.4556  0.4251  0.3850  0.3032\n",
       "  0.6005  0.6162  0.6186  0.3114  0.7274  0.5923  0.6403  0.5660  0.4083\n",
       "  0.3317  0.4536  0.3863  0.5675  0.4496  0.5385  0.3151  0.7018  0.6295\n",
       "  0.5408  0.4709  0.5192  0.5027  0.3398  0.5284  0.6194  0.5029  0.2889\n",
       "  0.3514  0.4948  0.4065  0.3527  0.6027  0.4945  0.6299  0.5090  0.4522\n",
       "  0.5432  0.4533  0.6574  0.5475  0.3808  0.4825  0.4612  0.6890  0.4969\n",
       "  0.6318  0.5665  0.5962  0.4131  0.5108  0.4693  0.4508  0.5037  0.4767\n",
       "  0.4235  0.6471  0.3512  0.6120  0.5699  0.4432  0.3423  0.4546  0.4167\n",
       "  0.3901  0.3364  0.5337  0.6500  0.6692  0.4609  0.4380  0.3829  0.5500\n",
       "  0.5859  0.5693  0.3383  0.4187  0.5671  0.6366  0.3462  0.5052  0.3378\n",
       "  0.5926  0.5605  0.6163  0.3510  0.6149  0.3259  0.3657  0.4414  0.3786\n",
       "  0.4666  0.5904  0.4648  0.5822  0.4621  0.5099  0.5612  0.5564  0.4316\n",
       "  0.6462  0.3788  0.6230  0.3621  0.7211  0.5183  0.5906  0.4005  0.4775\n",
       "  0.5265  0.5149  0.6164  0.4378  0.6522  0.5362  0.6127  0.5198  0.4371\n",
       "  0.4542  0.5045  0.4300  0.5623  0.5315  0.5181  0.6408  0.6370  0.5841\n",
       "  0.7099  0.5609  0.4186  0.4648  0.5900  0.4046  0.5104  0.3572  0.5738\n",
       "  0.4891  0.5122  0.6517  0.4687  0.5182  0.6244  0.3267  0.6141  0.6112\n",
       "  0.3932  0.6745  0.4379  0.4805  0.5089  0.4453  0.6350  0.5224  0.3455\n",
       "  0.5214  0.5623  0.6790  0.6262  0.3748  0.4952  0.3763  0.4836  0.3013\n",
       "  0.6515  0.6316  0.3903  0.6982  0.4747  0.6093  0.5257  0.4102  0.3358\n",
       "  0.6340  0.4778  0.4124  0.5250  0.4462  0.4129  0.5413  0.6259  0.5398\n",
       "  0.4759  0.5597  0.4616  0.4635  0.4927  0.6133  0.5301  0.5368  0.4003\n",
       "  0.6627  0.4782  0.2510  0.6009  0.4292  0.3303  0.4622  0.6260  0.3777\n",
       "  0.5593  0.5015  0.4741  0.4515  0.5218  0.5777  0.4263  0.3014  0.5014\n",
       "  0.6079  0.3462  0.4400  0.3782  0.5468  0.4712  0.4891  0.5778  0.4380\n",
       "  0.6337  0.2890  0.4207  0.4022  0.4893  0.4540  0.3854  0.5828  0.5772\n",
       "  0.4964  0.3258  0.5272  0.4259  0.3077  0.4907  0.5650  0.6857  0.5035\n",
       "  0.5438  0.6496  0.6482  0.5256  0.6438  0.3222  0.4684  0.6246  0.4441\n",
       "\n",
       "Columns 27 to 27 \n",
       "   0.6418\n",
       "  0.5280\n",
       "  0.4757\n",
       "  0.4446\n",
       "  0.5641\n",
       "  0.5747\n",
       "  0.6633\n",
       "  0.3865\n",
       "  0.4092\n",
       "  0.2575\n",
       "  0.5316\n",
       "  0.5255\n",
       "  0.5059\n",
       "  0.5454\n",
       "  0.7061\n",
       "  0.4586\n",
       "  0.4224\n",
       "  0.4970\n",
       "  0.4943\n",
       "  0.4874\n",
       "  0.5066\n",
       "  0.5216\n",
       "  0.6539\n",
       "  0.5303\n",
       "  0.4104\n",
       "  0.3934\n",
       "  0.6213\n",
       "  0.3953\n",
       "[torch.FloatTensor of size 1x1x28x28]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_unl_loader.dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):####can improve\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)####tune to improve\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CPU only training\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        #data = Noise.GaussianNoise(data)#add adaptive Gaussian Noises\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test(epoch, valid_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(valid_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(valid_loader.dataset),\n",
    "        100. * correct / len(valid_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3000 (0%)]\tLoss: 2.286879\n",
      "Train Epoch: 1 [640/3000 (21%)]\tLoss: 2.334832\n",
      "Train Epoch: 1 [1280/3000 (43%)]\tLoss: 2.309296\n",
      "Train Epoch: 1 [1920/3000 (64%)]\tLoss: 2.283294\n",
      "Train Epoch: 1 [2560/3000 (85%)]\tLoss: 2.300826\n",
      "\n",
      "Test set: Average loss: 2.2715, Accuracy: 2198/10000 (22%)\n",
      "\n",
      "Train Epoch: 2 [0/3000 (0%)]\tLoss: 2.302184\n",
      "Train Epoch: 2 [640/3000 (21%)]\tLoss: 2.272326\n",
      "Train Epoch: 2 [1280/3000 (43%)]\tLoss: 2.263016\n",
      "Train Epoch: 2 [1920/3000 (64%)]\tLoss: 2.286039\n",
      "Train Epoch: 2 [2560/3000 (85%)]\tLoss: 2.266297\n",
      "\n",
      "Test set: Average loss: 2.1984, Accuracy: 3739/10000 (37%)\n",
      "\n",
      "Train Epoch: 3 [0/3000 (0%)]\tLoss: 2.264426\n",
      "Train Epoch: 3 [640/3000 (21%)]\tLoss: 2.223513\n",
      "Train Epoch: 3 [1280/3000 (43%)]\tLoss: 2.149489\n",
      "Train Epoch: 3 [1920/3000 (64%)]\tLoss: 2.193071\n",
      "Train Epoch: 3 [2560/3000 (85%)]\tLoss: 2.074207\n",
      "\n",
      "Test set: Average loss: 1.9471, Accuracy: 4607/10000 (46%)\n",
      "\n",
      "Train Epoch: 4 [0/3000 (0%)]\tLoss: 1.978141\n",
      "Train Epoch: 4 [640/3000 (21%)]\tLoss: 2.027267\n",
      "Train Epoch: 4 [1280/3000 (43%)]\tLoss: 1.773699\n",
      "Train Epoch: 4 [1920/3000 (64%)]\tLoss: 1.787738\n",
      "Train Epoch: 4 [2560/3000 (85%)]\tLoss: 1.718247\n",
      "\n",
      "Test set: Average loss: 1.3868, Accuracy: 7065/10000 (71%)\n",
      "\n",
      "Train Epoch: 5 [0/3000 (0%)]\tLoss: 1.686819\n",
      "Train Epoch: 5 [640/3000 (21%)]\tLoss: 1.680416\n",
      "Train Epoch: 5 [1280/3000 (43%)]\tLoss: 1.387686\n",
      "Train Epoch: 5 [1920/3000 (64%)]\tLoss: 1.328931\n",
      "Train Epoch: 5 [2560/3000 (85%)]\tLoss: 1.403696\n",
      "\n",
      "Test set: Average loss: 0.8773, Accuracy: 8113/10000 (81%)\n",
      "\n",
      "Train Epoch: 6 [0/3000 (0%)]\tLoss: 1.369518\n",
      "Train Epoch: 6 [640/3000 (21%)]\tLoss: 1.242902\n",
      "Train Epoch: 6 [1280/3000 (43%)]\tLoss: 1.262246\n",
      "Train Epoch: 6 [1920/3000 (64%)]\tLoss: 1.106172\n",
      "Train Epoch: 6 [2560/3000 (85%)]\tLoss: 1.070311\n",
      "\n",
      "Test set: Average loss: 0.6633, Accuracy: 8396/10000 (84%)\n",
      "\n",
      "Train Epoch: 7 [0/3000 (0%)]\tLoss: 0.884750\n",
      "Train Epoch: 7 [640/3000 (21%)]\tLoss: 1.020532\n",
      "Train Epoch: 7 [1280/3000 (43%)]\tLoss: 1.047352\n",
      "Train Epoch: 7 [1920/3000 (64%)]\tLoss: 0.863420\n",
      "Train Epoch: 7 [2560/3000 (85%)]\tLoss: 0.794391\n",
      "\n",
      "Test set: Average loss: 0.5299, Accuracy: 8716/10000 (87%)\n",
      "\n",
      "Train Epoch: 8 [0/3000 (0%)]\tLoss: 0.810886\n",
      "Train Epoch: 8 [640/3000 (21%)]\tLoss: 0.932598\n",
      "Train Epoch: 8 [1280/3000 (43%)]\tLoss: 0.826992\n",
      "Train Epoch: 8 [1920/3000 (64%)]\tLoss: 0.783754\n",
      "Train Epoch: 8 [2560/3000 (85%)]\tLoss: 0.712829\n",
      "\n",
      "Test set: Average loss: 0.4686, Accuracy: 8737/10000 (87%)\n",
      "\n",
      "Train Epoch: 9 [0/3000 (0%)]\tLoss: 0.893210\n",
      "Train Epoch: 9 [640/3000 (21%)]\tLoss: 0.442107\n",
      "Train Epoch: 9 [1280/3000 (43%)]\tLoss: 0.469603\n",
      "Train Epoch: 9 [1920/3000 (64%)]\tLoss: 0.903244\n",
      "Train Epoch: 9 [2560/3000 (85%)]\tLoss: 1.010560\n",
      "\n",
      "Test set: Average loss: 0.4058, Accuracy: 8889/10000 (89%)\n",
      "\n",
      "Train Epoch: 10 [0/3000 (0%)]\tLoss: 0.704731\n",
      "Train Epoch: 10 [640/3000 (21%)]\tLoss: 0.663168\n",
      "Train Epoch: 10 [1280/3000 (43%)]\tLoss: 0.722480\n",
      "Train Epoch: 10 [1920/3000 (64%)]\tLoss: 0.819055\n",
      "Train Epoch: 10 [2560/3000 (85%)]\tLoss: 0.766156\n",
      "\n",
      "Test set: Average loss: 0.3803, Accuracy: 8913/10000 (89%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train(epoch)\n",
    "    test(epoch, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_unl_loader.batch_size\n",
    "len(train_unl_loader.dataset.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, \n",
       " ( 0 ,.,.) = \n",
       "   0.5542  0.4120  0.4187  ...   0.7066  0.4529  0.4194\n",
       " \n",
       " ( 1 ,.,.) = \n",
       "   0.5716  0.4887  0.3700  ...   0.7167  0.4359  0.4321\n",
       " \n",
       " ( 2 ,.,.) = \n",
       "   0.5621  0.4437  0.3946  ...   0.7014  0.4548  0.4605\n",
       " ... \n",
       " \n",
       " (61 ,.,.) = \n",
       "   0.6282  0.5878  0.3196  ...   0.5776  0.5457  0.5419\n",
       " \n",
       " (62 ,.,.) = \n",
       "   0.4259  0.4468  0.3233  ...   0.7559  0.5211  0.4210\n",
       " \n",
       " (63 ,.,.) = \n",
       "   0.6111  0.4168  0.3968  ...   0.6848  0.4354  0.4241\n",
       " [torch.FloatTensor of size 64x1x784])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(rec_unl_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1824\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.7269\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.0933  2.2105\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.8923  2.7833\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.4286  2.6306  2.7833\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.6577  2.7833  2.7833\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.6505  2.7833  2.7833\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  2.4524  2.7960  2.7960\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.8613  2.7451  2.7833  2.7833\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.0141  2.7833  2.7833  2.7833\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.0141  2.7833  2.7833  2.7833\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.0141  2.7833  2.7833  2.7833\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  2.4524  2.7960  2.7960\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  2.4396  2.7833  2.7833\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  1.0523  2.7833  2.7833\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.0678  2.1087  2.7833\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.1060  1.0013\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 9 to 17 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.0933  2.0069  1.0141  2.8215  2.7960  2.7960\n",
      " -0.4242 -0.4242 -0.0551  2.1214  2.7833  2.7833  2.7960  2.7833  2.7833\n",
      " -0.4242 -0.0933  2.1214  2.7833  2.7833  2.7833  2.7960  2.7833  2.7833\n",
      "  0.9377  2.2105  2.7833  2.7833  2.7833  2.7833  2.7960  2.7833  2.7833\n",
      "  2.7833  2.7960  2.7833  2.7833  2.7833  2.7833  2.7960  1.1923  1.0013\n",
      "  2.7960  2.8215  2.7960  2.7960  2.7960  1.9942 -0.4242 -0.4242 -0.4242\n",
      "  2.7833  2.7960  2.4142  0.3140 -0.0806 -0.2715 -0.4242 -0.4242 -0.4242\n",
      "  2.7833  2.7960  1.7014 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7833  2.7960  0.5177 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7833  2.7960 -0.0806 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7960  0.5813 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7833 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7833 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7833 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.2969  0.3395\n",
      "  2.7833 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242  0.6577  2.7833\n",
      "  2.7960  2.0196  0.5304 -0.0933  1.0141  2.0069  2.8215  2.7960  2.7960\n",
      "  2.7833  2.7960  2.6687  2.5160  2.7833  2.7833  2.7960  2.7833  2.7833\n",
      "  2.7833  2.7960  2.7833  2.7833  2.7833  2.7833  2.7960  2.7833  2.7833\n",
      "  2.7833  2.7960  2.7833  2.7833  2.7833  2.7833  2.6306  1.8160  0.6450\n",
      "  1.0013  2.7960  2.7833  2.7833  2.5797  1.0013  0.3777 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 18 to 26 \n",
      "  -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7960  2.7960  1.0268 -0.2715 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7833  2.7833  2.7960  1.5105 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7833  2.7833  2.7960  2.5415  0.5304 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7833  2.7833  2.7960  2.7833  2.1469 -0.2969 -0.4242 -0.4242 -0.4242\n",
      "  2.1851  2.7833  2.7960  2.7833  2.7833  0.6450 -0.4242 -0.4242 -0.4242\n",
      " -0.0551  2.7960  2.8215  2.7960  2.7960  0.6450 -0.4242 -0.4242 -0.4242\n",
      " -0.1824  2.2996  2.7960  2.7833  2.7833  2.2360 -0.4242 -0.4242 -0.4242\n",
      " -0.4242  1.3577  2.7960  2.7833  2.7833  2.4142 -0.4242 -0.4242 -0.4242\n",
      " -0.4242  1.3577  2.7960  2.7833  2.7833  2.2233 -0.4242 -0.4242 -0.4242\n",
      " -0.4242  1.3577  2.7960  2.7833  2.7833  0.6450 -0.4242 -0.4242 -0.4242\n",
      " -0.4242  1.3705  2.8215  2.7960  2.7960  0.6450 -0.4242 -0.4242 -0.4242\n",
      " -0.2969  1.8414  2.7960  2.7833  2.1342 -0.3097 -0.4242 -0.4242 -0.4242\n",
      "  1.3323  2.7833  2.7960  2.7833  2.0578 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.4269  2.7833  2.7960  2.7833  1.8669 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7833  2.7833  2.7960  2.1723 -0.1060 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7960  2.7960  2.1723  0.7722 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.7833  2.7833 -0.2333 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "  2.0960  0.2758 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.0678 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      " -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242 -0.4242\n",
      "\n",
      "Columns 27 to 27 \n",
      "  -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      " -0.4242\n",
      "[torch.FloatTensor of size 1x28x28]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, (data,target) in enumerate(train_unl_loader):\n",
    "    print(data[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.5335  0.4836  0.2274  0.3537  0.3119  0.5831  0.4096  0.4572  0.5225\n",
      "  0.2490  0.6028  0.4378  0.4671  0.6842  0.6516  0.2662  0.5216  0.4212\n",
      "  0.5561  0.5995  0.5136  0.6251  0.4179  0.6973  0.4954  0.5705  0.3481\n",
      "  0.4655  0.6654  0.6571  0.6203  0.4649  0.6675  0.3329  0.5135  0.5596\n",
      "  0.3470  0.5391  0.3722  0.5539  0.6951  0.3925  0.7076  0.6224  0.6059\n",
      "  0.5418  0.6224  0.4284  0.4600  0.5903  0.6538  0.3930  0.5804  0.3597\n",
      "  0.5174  0.5734  0.5511  0.6694  0.3567  0.5176  0.5208  0.4144  0.5192\n",
      "  0.5168  0.4466  0.3260  0.4771  0.2681  0.4995  0.4555  0.5913  0.4616\n",
      "  0.4116  0.3095  0.5996  0.4478  0.4256  0.5943  0.4866  0.4326  0.6170\n",
      "  0.3752  0.5098  0.4847  0.4071  0.4758  0.5885  0.4319  0.5428  0.4881\n",
      "  0.4813  0.3900  0.6634  0.4956  0.4745  0.4345  0.6089  0.3543  0.4658\n",
      "  0.4598  0.4560  0.3598  0.5576  0.3632  0.4484  0.4777  0.3592  0.3565\n",
      "  0.5236  0.3542  0.6495  0.5818  0.6323  0.6409  0.3705  0.4180  0.4844\n",
      "  0.5839  0.6169  0.6228  0.6232  0.3982  0.4978  0.4597  0.6374  0.5894\n",
      "  0.5031  0.5381  0.2548  0.5363  0.4840  0.5359  0.4707  0.4775  0.5042\n",
      "  0.4620  0.4413  0.6006  0.5475  0.4216  0.6624  0.5791  0.6201  0.3845\n",
      "  0.3631  0.6137  0.4501  0.4942  0.5605  0.5355  0.4066  0.6065  0.4217\n",
      "  0.4572  0.3573  0.5077  0.2585  0.3245  0.3785  0.5672  0.3047  0.3392\n",
      "  0.4594  0.2773  0.3224  0.4561  0.4644  0.5571  0.3589  0.3408  0.4677\n",
      "  0.4802  0.5388  0.2977  0.5966  0.4747  0.6085  0.5248  0.3507  0.4946\n",
      "  0.5729  0.2695  0.3526  0.3771  0.6916  0.6009  0.4997  0.5728  0.5740\n",
      "  0.3857  0.6127  0.5004  0.5787  0.4926  0.5661  0.6346  0.5075  0.5786\n",
      "  0.4751  0.6983  0.3672  0.3563  0.4843  0.1452  0.5232  0.6772  0.4192\n",
      "  0.4806  0.7475  0.4727  0.3675  0.4067  0.6698  0.4652  0.5901  0.6553\n",
      "  0.3958  0.5620  0.5851  0.6714  0.4384  0.6959  0.7052  0.6951  0.5275\n",
      "  0.5427  0.4926  0.5368  0.4370  0.5315  0.3114  0.5969  0.6154  0.5621\n",
      "  0.3897  0.4864  0.5348  0.5600  0.4660  0.3729  0.4079  0.3809  0.3711\n",
      "  0.4926  0.5359  0.3885  0.3366  0.5124  0.5649  0.4665  0.6192  0.5520\n",
      "\n",
      "Columns 9 to 17 \n",
      "   0.3867  0.5638  0.3457  0.6278  0.5307  0.3957  0.6017  0.6499  0.4149\n",
      "  0.5643  0.6616  0.4556  0.3342  0.4656  0.4473  0.4638  0.5003  0.4483\n",
      "  0.4747  0.6042  0.5854  0.6334  0.3093  0.5158  0.5756  0.5934  0.5307\n",
      "  0.5311  0.7292  0.4965  0.5084  0.5866  0.4794  0.3633  0.5665  0.5533\n",
      "  0.4966  0.5252  0.6929  0.5575  0.5325  0.4284  0.5109  0.5506  0.5468\n",
      "  0.4051  0.4212  0.4382  0.6781  0.6657  0.5624  0.6932  0.5480  0.2660\n",
      "  0.4886  0.5837  0.6262  0.3644  0.6033  0.5820  0.5621  0.3819  0.2837\n",
      "  0.6970  0.6085  0.4862  0.5425  0.5741  0.4581  0.5079  0.5389  0.4197\n",
      "  0.3486  0.5880  0.2700  0.5627  0.5696  0.3718  0.6434  0.6038  0.6730\n",
      "  0.4138  0.5182  0.3653  0.5558  0.5001  0.4165  0.4519  0.4279  0.5396\n",
      "  0.5142  0.4267  0.6200  0.3403  0.3221  0.4086  0.3275  0.4637  0.4323\n",
      "  0.3499  0.6947  0.3327  0.4934  0.6076  0.5257  0.3731  0.5333  0.3502\n",
      "  0.5596  0.5275  0.3930  0.6373  0.5920  0.5899  0.4936  0.2637  0.3030\n",
      "  0.5658  0.4783  0.3891  0.5658  0.3288  0.3160  0.5847  0.5416  0.4134\n",
      "  0.5105  0.6387  0.4883  0.4931  0.4545  0.5216  0.4177  0.4045  0.6984\n",
      "  0.4744  0.5264  0.3516  0.5979  0.3696  0.5981  0.4152  0.7490  0.4535\n",
      "  0.5467  0.3874  0.4102  0.4795  0.2640  0.5166  0.5738  0.5621  0.4814\n",
      "  0.4118  0.4314  0.3992  0.5051  0.4352  0.4643  0.5193  0.5364  0.4242\n",
      "  0.4618  0.5269  0.3823  0.6682  0.3354  0.5923  0.5263  0.6513  0.5319\n",
      "  0.6529  0.5675  0.5343  0.3679  0.6728  0.7002  0.5267  0.5921  0.4570\n",
      "  0.4201  0.5516  0.4379  0.3138  0.5325  0.8026  0.6094  0.6340  0.3732\n",
      "  0.5160  0.5325  0.3746  0.6009  0.4988  0.4548  0.5037  0.4079  0.3675\n",
      "  0.5567  0.6436  0.6254  0.4942  0.5615  0.6494  0.5863  0.5053  0.5043\n",
      "  0.6768  0.5089  0.4474  0.6469  0.6731  0.5133  0.3813  0.5279  0.5736\n",
      "  0.5026  0.5949  0.4688  0.5760  0.5166  0.8115  0.4852  0.3138  0.3776\n",
      "  0.2508  0.4084  0.4257  0.6144  0.4054  0.6435  0.4902  0.5568  0.5495\n",
      "  0.4907  0.4241  0.5204  0.5165  0.4923  0.5159  0.5509  0.5779  0.5934\n",
      "  0.4191  0.3628  0.5561  0.5193  0.4307  0.6306  0.7466  0.5166  0.5315\n",
      "\n",
      "Columns 18 to 26 \n",
      "   0.5493  0.5119  0.4906  0.3983  0.2704  0.4996  0.4591  0.2851  0.3577\n",
      "  0.5375  0.6277  0.5771  0.3472  0.7239  0.5413  0.5990  0.6047  0.3926\n",
      "  0.3296  0.4190  0.4538  0.5912  0.4365  0.5784  0.2887  0.6621  0.6253\n",
      "  0.4754  0.4685  0.5609  0.4982  0.2768  0.4352  0.6316  0.5556  0.2723\n",
      "  0.3831  0.5117  0.3575  0.3789  0.5312  0.4404  0.5300  0.5345  0.5059\n",
      "  0.5343  0.4551  0.6843  0.5074  0.3268  0.4440  0.4973  0.7189  0.4759\n",
      "  0.5702  0.5719  0.6016  0.4459  0.4610  0.5226  0.3949  0.5707  0.4262\n",
      "  0.4381  0.7052  0.4027  0.5077  0.5894  0.4811  0.3697  0.4469  0.4024\n",
      "  0.4644  0.3971  0.6237  0.6186  0.6364  0.5535  0.4966  0.3901  0.4926\n",
      "  0.6098  0.5736  0.4115  0.4321  0.6152  0.5453  0.3474  0.5399  0.2973\n",
      "  0.5625  0.5615  0.5504  0.3755  0.5476  0.3136  0.4571  0.4380  0.4099\n",
      "  0.4744  0.6277  0.5036  0.6439  0.5071  0.5405  0.4721  0.5779  0.4413\n",
      "  0.6238  0.3871  0.5507  0.4254  0.7156  0.5726  0.5584  0.4151  0.5787\n",
      "  0.5757  0.5039  0.6634  0.4685  0.6670  0.5057  0.6027  0.4306  0.4317\n",
      "  0.5748  0.4526  0.4492  0.6304  0.4606  0.5503  0.5876  0.6648  0.6326\n",
      "  0.6727  0.5038  0.3931  0.5375  0.5497  0.4474  0.5706  0.3388  0.5493\n",
      "  0.4514  0.5394  0.6034  0.4709  0.4638  0.6320  0.3778  0.6002  0.6347\n",
      "  0.3863  0.6541  0.4560  0.5223  0.5416  0.4074  0.6664  0.4893  0.3641\n",
      "  0.5662  0.5811  0.6549  0.6915  0.4201  0.5121  0.4303  0.4444  0.3126\n",
      "  0.6690  0.5540  0.3715  0.6060  0.5215  0.5500  0.5362  0.3661  0.2931\n",
      "  0.6563  0.4751  0.3762  0.5405  0.5013  0.3640  0.5188  0.6171  0.5669\n",
      "  0.5593  0.5899  0.4745  0.5159  0.5110  0.5599  0.5721  0.5355  0.4035\n",
      "  0.6769  0.4902  0.2471  0.6292  0.4207  0.4545  0.5447  0.6735  0.4170\n",
      "  0.6274  0.5667  0.4924  0.3437  0.5836  0.5134  0.4598  0.3576  0.4764\n",
      "  0.5483  0.2965  0.4736  0.3514  0.5255  0.4888  0.5389  0.6443  0.4560\n",
      "  0.5858  0.3211  0.4395  0.4678  0.5515  0.4876  0.4463  0.6267  0.5840\n",
      "  0.4532  0.3896  0.5309  0.4761  0.3688  0.4606  0.5574  0.6704  0.5444\n",
      "  0.4709  0.5800  0.6281  0.4259  0.6523  0.3714  0.5427  0.6626  0.4689\n",
      "\n",
      "Columns 27 to 27 \n",
      "   0.6031\n",
      "  0.5183\n",
      "  0.5549\n",
      "  0.4509\n",
      "  0.5633\n",
      "  0.6030\n",
      "  0.6278\n",
      "  0.4156\n",
      "  0.3772\n",
      "  0.3151\n",
      "  0.5324\n",
      "  0.5404\n",
      "  0.6036\n",
      "  0.4573\n",
      "  0.7601\n",
      "  0.4488\n",
      "  0.4859\n",
      "  0.5019\n",
      "  0.5267\n",
      "  0.5321\n",
      "  0.4861\n",
      "  0.4798\n",
      "  0.6293\n",
      "  0.5716\n",
      "  0.3783\n",
      "  0.4697\n",
      "  0.6500\n",
      "  0.4496\n",
      "[torch.FloatTensor of size 1x1x28x28]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(rec_unl_loader):\n",
    "    print(data[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "semi-supervised learning\n",
    "\"\"\"\n",
    "time = 0\n",
    "\n",
    "def alpha(t):\n",
    "    alpha_f = 3.\n",
    "    T1 = 100\n",
    "    T2 = 600\n",
    "    if t < T1:\n",
    "        return 0\n",
    "    elif t < T2:\n",
    "        return alpha_f * (t-T1) / (T2-T1)\n",
    "    else:\n",
    "        return alpha_f\n",
    "\n",
    "def output2p_label(output):\n",
    "    \"\"\"\n",
    "    get the pseudo label from the model output \n",
    "    \n",
    "    @param output: a Variable\n",
    "    @return indices: a Variable, squeezed\n",
    "    \"\"\"\n",
    "    y_i, indices = torch.max(output, 1)\n",
    "    indices = indices.squeeze()\n",
    "    indices = Variable(indices.data)\n",
    "    return indices\n",
    "    \n",
    "def train_semi(epoch):\n",
    "    global time\n",
    "    model.train()\n",
    "    \n",
    "    n_labeled = len(train_loader.dataset.train_data)\n",
    "    n_unlabeled = len(train_unl_loader.dataset.train_data)\n",
    "    labeled_batch_size = train_loader.batch_size\n",
    "    unlabeled_batch_size = train_unl_loader.batch_size\n",
    "\n",
    "    # because #label < #unlabel, use round-robin for labeled data\n",
    "    labeled_batches = int(np.ceil(1. * n_labeled / labeled_batch_size))\n",
    "    batches = int(np.ceil(1. * n_unlabeled / unlabeled_batch_size))\n",
    "\n",
    "    # locally allocate memory to store labeled data list, reduce overhead\n",
    "    label_list = list(enumerate(train_loader))\n",
    "\n",
    "    # unlabeled data has no label, enumeration causes error\n",
    "    # so manually add 0's for convenience [ignore the label anyway]\n",
    "    train_unl_loader.dataset.train_labels = torch.zeros(n_unlabeled)\n",
    "    unlabel_list = list(enumerate(rec_unl_loader))\n",
    "    \n",
    "    # semi-supervised learning both labeled and unlabeled data\n",
    "    for batch_id in range(batches):\n",
    "        optimizer.zero_grad()\n",
    "        time += 1\n",
    "        \n",
    "        # get labeled data to supervised learn\n",
    "        idx, (data, target) = label_list[batch_id % labeled_batches]\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        output = model(data)\n",
    "        labeled_loss = criterion(output, target)\n",
    "\n",
    "        # get unlabeled data to unsupervised learn\n",
    "        idx, data = unlabel_list[batch_id]\n",
    "        data = Variable(data)\n",
    "        output = model(data)\n",
    "        pseudo_label = output2p_label(output)\n",
    "        unlabeled_loss = criterion(output, pseudo_label)\n",
    "        \n",
    "        # sum the losses\n",
    "        loss = labeled_loss + alpha(time) * unlabeled_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        if batch_id % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_id * len(data), len(train_unl_loader.dataset),\n",
    "                100. * batch_id / len(train_unl_loader), loss.data[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-719ca47f6c36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_semi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-adca90aad0f4>\u001b[0m in \u001b[0;36mtrain_semi\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munlabel_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mpseudo_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput2p_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0munlabeled_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpseudo_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/twff/anaconda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-15c3595537f3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m####can improve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/twff/anaconda/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/twff/anaconda/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 235\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/twff/anaconda/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     35\u001b[0m     f = ConvNd(_pair(stride), _pair(padding), _pair(dilation), False,\n\u001b[1;32m     36\u001b[0m                _pair(0), groups)\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/twff/anaconda/lib/python3.5/site-packages/torch/nn/_functions/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_view4d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_view3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/twff/anaconda/lib/python3.5/site-packages/torch/nn/_functions/conv.py\u001b[0m in \u001b[0;36m_update_output\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bufs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'update_output'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_grad_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/twff/anaconda/lib/python3.5/site-packages/torch/nn/_functions/conv.py\u001b[0m in \u001b[0;36m_thnn\u001b[0;34m(self, fn_name, input, weight, *args)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_thnn_convs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn_class_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bufs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/twff/anaconda/lib/python3.5/site-packages/torch/nn/_functions/conv.py\u001b[0m in \u001b[0;36mcall_update_output\u001b[0;34m(self, bufs, input, weight, bias)\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype2backend\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mbufs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/twff/anaconda/lib/python3.5/site-packages/torch/nn/_functions/conv.py\u001b[0m in \u001b[0;36m_output_size\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0min_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mpad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "for test and debugging\n",
    "\"\"\"\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    train_semi(epoch)\n",
    "    test(epoch, valid_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sample Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testset = pickle.load(open(\"test.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(testset,batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 7\n",
       " 2\n",
       " 1\n",
       " \n",
       " 4\n",
       " 5\n",
       " 6\n",
       "[torch.LongTensor of size 10000]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test(1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_predict = np.array([])\n",
    "model.eval()\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    output = model(data)\n",
    "    temp = output.data.max(1)[1].numpy().reshape(-1)\n",
    "    label_predict = np.concatenate((label_predict, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.,  2.,  1., ...,  4.,  5.,  6.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_true = test_loader.dataset.test_labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diff_array = label_true - label_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(diff_array != 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "true_label = pd.DataFrame(label_true, columns=['label'])\n",
    "true_label.reset_index(inplace=True)\n",
    "true_label.rename(columns={'index': 'ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  label\n",
       "0   0      7\n",
       "1   1      2\n",
       "2   2      1\n",
       "3   3      0\n",
       "4   4      4"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_label = pd.DataFrame(label_predict, columns=['label'], dtype=int)\n",
    "predict_label.reset_index(inplace=True)\n",
    "predict_label.rename(columns={'index': 'ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  label\n",
       "0   0      7\n",
       "1   1      2\n",
       "2   2      1\n",
       "3   3      0\n",
       "4   4      4\n",
       "5   5      1\n",
       "6   6      4\n",
       "7   7      9\n",
       "8   8      5\n",
       "9   9      9"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_label.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_label.to_csv('sample_submission.csv', index=False)\n",
    "true_label.to_csv('true_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
